# -*- coding: utf-8 -*-
"""Final Report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12VLlO4gJp14nIa0mob54NJSZWWCmeS_1

**----------------------------------------STUDENT INFORMATION----------------------------------------**
"""

print('Name: Joe Tiwari             ID: 1202029')
print('Name: Jason Yang             ID: 1011881')
print('Name: Ashton Schneider       ID: 1227445')

"""# Importing files, libraries"""

# From google.colab import files
# Uploaded = files.upload()
import io
import pandas as pd
import numpy as np
import requests
import seaborn as sns
import nltk
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import confusion_matrix
from sklearn import metrics

from requests.structures import CaseInsensitiveDict
from wordcloud import WordCloud
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords

!wget 'https://raw.githubusercontent.com/adrawnangel/AIprojectfinal/main/spam.csv'
#df = pd.read_csv(url), encoding="ISO-8859-1"
#df = pd.read_csv(io.BytesIO(uploaded['spam_texts.csv']), encoding="ISO-8859-1")

df = pd.read_csv('/content/spam.csv', encoding="ISO-8859-1")

"""## Cleaning Up The Data

"""

df.head()

df.columns

#df = df.drop(columns = ['Unnamed: 2','Unnamed: 3', 'Unnamed: 4' ] )
df.drop(df.columns[[2,3,4]], axis=1, inplace=True)
df.rename(columns = {'v1': 'target', 'v2':'message'}, inplace = True)

df.head(10)

df.isna().sum()

df.dropna(inplace=True)
df.isnull().sum()

df.duplicated().sum()

df = df.drop_duplicates(keep = "first")
df.duplicated().sum()

# Count number of ham and spam
df['target'].value_counts()

# Converting ham and spam into numeric: 0 is ham, 1 is spam
le = LabelEncoder()
df['target'] = le.fit_transform(df['target'])
df.head(10)

"""## EDA"""

print('0:Ham \n1:Spam')
df['target'].value_counts()

# Pie graph of ham and spam messages
plt.pie(df['target'].value_counts(), labels=['ham', 'spam'], autopct='%1.1f%%')
plt.show()

# Find number of characters in each message
df['length'] = df['message'].apply(len)
# Find number of words in each message
df['word_count'] = df['message'].apply(lambda x: len(str(x).split(" ")))
# Find number of sentences in each message
df['sentence_count'] = df['message'].apply(lambda x: len(str(x).split(".")))

# Describe the word_count, sentence_count, special_char_count for ham
df[df['target']==0][['word_count', 'sentence_count']].describe()

# Describe the word_count, sentence_count, special_char_count for spam
df[df['target']==1][['word_count', 'sentence_count']].describe()

# You can see that spam char and words and sentences counts are alot higher than ham messages

"""##Preprocessing Data
1. LowerCase
2. Tokenization (splitting up a larger body of text into smaller lines, words)
3. Removing Special Characters(EX: stop,the,is,to)
4. Remove Punctuations and stop words
5. Stemming Remove similar sounding words (Ex: speaking, spoke to just Speak)

"""

# Lowercase the dataset
df['message'] = df['message'].apply(lambda x: " ".join(x.lower() for x in x.split()))
df['message'].head(10)

# Tokenize the dataset
nltk.download('punkt')

df['message'] = df['message'].apply(word_tokenize)
df['message'].head(10)

# Remove special characters
df['message'] = df['message'].apply(lambda x: [word for word in x if word.isalpha()])
df['message'].head(10)

# Remove Punctuations and stop words
nltk.download('stopwords')
stop = stopwords.words('english')

df['message'] = df['message'].apply(lambda x: [word for word in x if word not in stop])
df['message'].head(10)

#stemming .... Remove similar sounding words (Ex: speaking, spoke to just Speak)
st = PorterStemmer()
df['message'] = df['message'].apply(lambda x: [st.stem(word) for word in x])
df['message'].head(10)

# Get rid of bracket []
df['message'] = df['message'].apply(lambda x: ' '.join(x))
df['message'].head(10)

df.head()

# Create wordcloud for spam messages
spam_words = ' '.join(list(df[df['target']==1]['message']))
spam_wc = WordCloud(width=300, height=300).generate(spam_words)
plt.figure(figsize=(6,6), facecolor='black')
plt.imshow(spam_wc)

#wordcloud for ham messages
ham_words = ' '.join(list(df[df['target']==0]['message']))
ham_wc = WordCloud(width=300,height=300).generate(ham_words)
plt.figure(figsize=(6,6),facecolor='black')
plt.imshow(ham_wc)

# Top 20 most used words in spam messages as a bar graph
spam_words = ' '.join(list(df[df['target']==1]['message']))
spam_words = [word for word in spam_words.split()]
spam_words = pd.Series(spam_words).value_counts()[:20]

plt.figure(figsize=(12,8))
sns.barplot(x=spam_words, y=spam_words.index)
plt.xlabel('Count')
plt.ylabel('Word')
plt.title('Top 20 most used words in spam messages')
plt.show()

# Top 20 most used words in ham messages as a bar graph
ham_words = ' '.join(list(df[df['target']==0]['message']))
ham_words = [word for word in ham_words.split()]
ham_words = pd.Series(ham_words).value_counts()[:20]

plt.figure(figsize=(12,8))
sns.barplot(x=ham_words, y=ham_words.index)
plt.xlabel('Count')
plt.ylabel('Word')
plt.title('Top 20 most used words in ham messages')
plt.show()

"""##Count Vectorizer Algorithm"""

# Converting text into vectors
cv = CountVectorizer(input='message', encoding='utf-8', lowercase=True, stop_words='english',
                     ngram_range=(1,2), max_features=5000, analyzer='word', max_df=0.8, min_df=2,
                     binary=True)
X = cv.fit_transform(df['message']).toarray()
y = df['target'].values

# Scale the data using MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Train the data
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size = 0.2,
    random_state = 32
)

# we did test using 30% test size and really had little to no change in the accuracy

# Train the model using MultinomialNB
model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print(f"Accuracy:{100 * accuracy: .3f}%")
print(f"Precision:{100 * precision: .3f}%")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# Predict a single message
message ="Congratulations you won... Click this link http:/1231 to claim your prize"
message = cv.transform([message]).toarray()
model.predict(message)

"""MultinomialNB have a better performance compared to Decision Tree Classifier"""

# Train the model using Decision Tree Classifier
dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)

y_pred = dtc.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print(f"Accuracy:{100 * accuracy: .3f}%")
print(f"Precision:{100 * precision: .3f}%")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# Predict a single message
message ="Congratulations you won... Click this link http:/1231 to claim your prize"
message = cv.transform([message]).toarray()
dtc.predict(message)

"""##Tfidf Vectorizer algorithm"""

tfidf = TfidfVectorizer(min_df=5, max_df=0.5, max_features=10000, ngram_range=(1, 3))
X = tfidf.fit_transform(df['message'])
y = df['target'].values

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size = 0.2,
    random_state = 32
)

model2 = MultinomialNB()
model2.fit(X_train, y_train)

y_pred = model2.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print(f"Accuracy:{100 * accuracy: .3f}%")
print(f"Precision:{100 * precision: .3f}%")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# Predict a single message
message ="Congratulations you won... Click this link http:/1231 to claim your prize"
message = tfidf.transform([message]).toarray()
model2.predict(message)

"""MultinomialNB have a better performance compared to Decision Tree Classifier"""

dtc2 = DecisionTreeClassifier()
dtc2.fit(X_train, y_train)

y_pred = dtc2.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print(f"Accuracy:{100 * accuracy: .3f}%")
print(f"Precision:{100 * precision: .3f}%")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# Predict a single message
message ="Congratulations you won... Click this link http:/1231 to claim your prize"
message = tfidf.transform([message]).toarray()
dtc2.predict(message)

"""#Testing a different dataset"""

!wget 'https://raw.githubusercontent.com/adrawnangel/AIprojectfinal/main/spam_ham_dataset.csv'

df2 = pd.read_csv('/content/spam_ham_dataset.csv', encoding="ISO-8859-1")

"""##Cleaning Up The Data"""

df2.head()

df2.columns

df2.drop(df2.columns[[0, 3]], axis=1, inplace=True)
df2.rename(columns = {'label': 'target', 'text':'message'}, inplace = True)

df2.head(10)

df2.isna().sum()

df2.dropna(inplace=True)
df2.isnull().sum()

df2.duplicated().sum()

df2 = df2.drop_duplicates(keep = "first")
df2.duplicated().sum()

# Count number of ham and spam
df2['target'].value_counts()

# Converting ham and spam into numeric: 0 is ham, 1 is spam
#le comes from LabelEncoder, that was already imported in the first dataset
df2['target'] = le.fit_transform(df2['target'])
df2.head(10)

"""## EDA"""

print('0:Ham \n1:Spam')
df2['target'].value_counts()

# Pie graph of ham and spam messages
import matplotlib.pyplot as plt

plt.pie(df2['target'].value_counts(), labels=['ham', 'spam'], autopct='%1.1f%%')
plt.show()

# Find number of characters in each message
df2['length'] = df2['message'].apply(len)
# Find number of words in each message
df2['word_count'] = df2['message'].apply(lambda x: len(str(x).split(" ")))
# Find number of sentences in each message
df2['sentence_count'] = df2['message'].apply(lambda x: len(str(x).split(".")))

# Describe the word_count, sentence_count for ham
df2[df2['target']==0][['word_count', 'sentence_count']].describe()

# Describe the word_count, sentence_count for spam
df2[df2['target']==1][['word_count', 'sentence_count']].describe()

"""##Preprocessing Data
1. LowerCase
2. Tokenization (splitting up a larger body of text into smaller lines, words)
3. Removing Special Characters(EX: stop,the,is,to)
4. Remove Punctuations and stop words
5. Stemming Remove similar sounding words (Ex: speaking, spoke to just Speak)

"""

# Lowercase the dataset
df2['message'] = df2['message'].apply(lambda x: " ".join(x.lower() for x in x.split()))
df2['message'].head(10)

# Tokenize the dataset
nltk.download('punkt')

df2['message'] = df2['message'].apply(word_tokenize)
df2['message'].head(10)

# Remove special characters
df2['message'] = df2['message'].apply(lambda x: [word for word in x if word.isalpha()])
df2['message'].head(10)

# Remove Punctuations and stop words
nltk.download('stopwords')
stop = stopwords.words('english')
stop.append('subject')

df2['message'] = df2['message'].apply(lambda x: [word for word in x if word not in stop])
df2['message'].head(10)

# stemming .... Remove similar sounding words (Ex: speaking, spoke to just Speak)
df2['message'] = df2['message'].apply(lambda x: [st.stem(word) for word in x])
df2['message'].head(10)

# Get rid of bracket []
df2['message'] = df2['message'].apply(lambda x: ' '.join(x))
df2['message'].head(10)

df2.head()

# Create wordcloud for spam messages
spam_words2 = ' '.join(list(df2[df2['target']==1]['message']))
spam_wc2 = WordCloud(width=300, height=300).generate(spam_words2)
plt.figure(figsize=(6,6), facecolor='black')
plt.imshow(spam_wc2)

# wordcloud for ham messages
ham_words2 = ' '.join(list(df2[df2['target']==0]['message']))
ham_wc2 = WordCloud(width=300,height=300).generate(ham_words2)
plt.figure(figsize=(6,6),facecolor='black')
plt.imshow(ham_wc2)

# Top 20 most used words in spam messages as a bar graph
spam_words2 = ' '.join(list(df2[df2['target']==1]['message']))
spam_words2 = [word for word in spam_words2.split()]
spam_words2 = pd.Series(spam_words2).value_counts()[:20]

plt.figure(figsize=(12,8))
sns.barplot(x=spam_words2, y=spam_words2.index)
plt.xlabel('Count')
plt.ylabel('Word')
plt.title('Top 20 most used words in spam messages')
plt.show()

# Top 20 most used words in ham messages as a bar graph
ham_words2 = ' '.join(list(df2[df2['target']==0]['message']))
ham_words2 = [word for word in ham_words2.split()]
ham_words2 = pd.Series(ham_words2).value_counts()[:20]

plt.figure(figsize=(12,8))
sns.barplot(x=ham_words2, y=ham_words2.index)
plt.xlabel('Count')
plt.ylabel('Word')
plt.title('Top 20 most used words in ham messages')
plt.show()

"""##Count Vectorizer Algorithm"""

cv2 = CountVectorizer(input='message', encoding='utf-8', lowercase=True, stop_words='english',
                     ngram_range=(1,2), max_features=5000, analyzer='word', max_df=0.8, min_df=2,
                     binary=True)

X = cv2.fit_transform(df2['message']).toarray()
y = df2['target'].values

# Scale the data using MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Trainning the data

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size = 0.2,
    random_state = 32
)

# Train the model using MultinomialNB
model3 = MultinomialNB()

model3.fit(X_train, y_train)

y_pred = model3.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print(f"Accuracy:{100 * accuracy: .3f}%")
print(f"Precision:{100 * precision: .3f}%")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# Predict a single message
message ="Hi friend please come over"
message = cv2.transform([message]).toarray()
model3.predict(message)

# Train the model using Decision Tree Classifier
dtc3 = DecisionTreeClassifier()

dtc3.fit(X_train, y_train)

y_pred = dtc3.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print(f"Accuracy:{100 * accuracy: .3f}%")
print(f"Precision:{100 * precision: .3f}%")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# Predict a single message
message ="Congratulations you won... Click this link http:/1231 to claim your prize"
message = cv2.transform([message]).toarray()
dtc3.predict(message)

"""##Tfidf Vectorizer algorithm"""

tfidf2 = TfidfVectorizer(min_df=5, max_df=0.5, max_features=10000, ngram_range=(1, 3))
X = tfidf2.fit_transform(df2['message'])
y = df2['target'].values

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size = 0.2,
    random_state = 32
)

# Train the model using MultinomialNB
model4 = MultinomialNB()
model4.fit(X_train, y_train)

y_pred2 = model4.predict(X_test)
accuracy = accuracy_score(y_test, y_pred2)
precision = precision_score(y_test, y_pred2)

print(f"Accuracy:{100 * accuracy: .3f}%")
print(f"Precision:{100 * precision: .3f}%")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred2))

# Predict a single message
message ="Hi friend"
message = tfidf2.transform([message]).toarray()
model4.predict(message)

# Train the model using Decision Tree Classifier
dtc4 = DecisionTreeClassifier()
dtc4.fit(X_train, y_train)

y_pred2 = dtc4.predict(X_test)
accuracy = accuracy_score(y_test, y_pred2)
precision = precision_score(y_test, y_pred2)

print(f"Accuracy:{100 * accuracy: .3f}%")
print(f"Precision:{100 * precision: .3f}%")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred2))

# Predict a single message
message ="hi"
message = tfidf2.transform([message]).toarray()
dtc4.predict(message)