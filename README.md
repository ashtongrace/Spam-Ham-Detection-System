# Spam-Ham-Detection-System
This project was created with a team of two others to implement a spam and ham detection system that implemented machine learning tools and the use of public ham and spam datasets.

Our datasets were both found via Kaggle. The first dataset we use, the SMS Spam Collection Dataset, comes from a larger set of SMS spam messages collected from a website known as Grumbletext, a UK forum where people make claims about different SMS spam messages. The identification of their text was garnered via scanning many webpages. Ham messages were found via the NSC or NUS SMS Corpus, a dataset of 10k legitimate messages collected for research purposes form the National University of Singapore’s computer science department. This data specifically came from Singaporean citizens, primarily students in the university. 

The second dataset, on the other hand, comes from the Enron-Spam datasets. This data was already somewhat preprocessed when we got it, with senders and virus-infected messages having already been removed for testing purposes. Spam messages in non-Latin encoding were also removed before we worked with the dataset, but it can be found in our own experimentation that we account for encoding errors by specifying our encoding when we import the appropriate files, may it be the first or second dataset. The second dataset’s data was collected for the purpose of writing a 2006 academic paper in which the question of which Naive Bayes model was most adequate to use was questioned. We only explore enron1, or one portion of the data in our testing, as this data was imported to Kaggle and that is how we found it, with the data already iterated into the data frame for us.

For the purpose of testing, CSVs were downloaded to a group member’s local computer, uploaded to Github, and converted to raw data so that it could be retrieved within the notebook. 

https://www2.aueb.gr/users/ion/docs/ceas2006_paper.pdf
https://www2.aueb.gr/users/ion/data/enron-spam/readme.txt 
https://www.kaggle.com/datasets/venky73/spam-mails-dataset 
https://www.kaggle.com/code/ishansoni/sms-spam-collection-dataset 

The following tasks were completed:

After loading the dataset, Our first step was to clean up the data as much as we can so that we can have the most accurate and efficient model. We started with getting rid of the unwanted columns, then we checked for any null values which in this dataset was 0, next we searched for any duplications and in fact there was 403 duplicates which we got rid off as well. So the total Ham number was 4516 and 653 spam. Finally, Using the LabelEncoder we converted the target which is spam/ham into 0 being ham and 1 being spam.

After cleaning up the data, our next step was the EDA (Exploratory Data Analysis). We started it off with a pie graph that gave us a visualization of how much ham and spam data our dataset contained (87% ham, 13% spam). Then, we moved into finding the number of characters, number of words and number of sentences. We used the describe() function to analyze the word count and sentence count for both ham and spam. We found the word count was much higher for our spam data than our ham data.

This led us to our one of the most important parts of making the system work, which was preprocessing data. Our first step on it was to "LowerCase" our text and Tokenize it using NLTK (Natural Language toolkit) library that contains the Tokenizer function, which splits a string of text into sequence of tokens or smaller substrings. The tokens can be individual words, phrases, or other meaningful elements of the text. Next, we removed special characters by using isalpha() function that only returns alphabetical characters. Moving on, our next step was to remove any punctuation and stop words by downloading stopwords from NLTK  which is a submodule of NLTK which helps to remove any stop words or punctuation. For the final step we removed similar sounding words so that we can have a very strong and accurate dataset. We achieved this by using submodule called PortStemmer from NLTK which works on removing the similar sounding words. (Ex: Speaking and spoke to just speak.) We then created a word cloud and a bar graph for most used words pertaining to each side of the system (both ham and spam words). 

CountVectorizer was employed for the purpose of converting text into a vector of tokens. Because we work with text classification in our datasets, it is necessary for us to convert our text into a form that our machine learning models can understand. We experimented with different parameters and used these parameters in both datasets. After we set up our count vectorizer, whether it was for dataset 1 or for dataset 2, our next job was to train the data. We set our test/train size to be an 80/20 split, or 80% training data and 20% testing data. Not only was that a parameter we had seen in homework assignments, so it was one we were familiar with, but with our research we found that it was the most commonly used ratio, so that is what we went with here. Our random state was set to 32. It is the random state we picked based off of a random number close to the student IDs of our group members, and we used it throughout our experimentation.

In our first dataset, using MultinomialNB generated an accuracy of 98.646% and a precision score of 96.262%. MultinomialNB is good for classification with discrete features such as text data / word count, which is why we experimented with using it. Our confusion matrix had a high true positive score, meaning a high rate of positive examples classified accurately, and a high true negative score, meaning a high rate of negative examples classified accurately and small false positive and false negative scores, which was a good sign. It meant that the model was doing well at classifying whether the data was a spam or ham message. We found a similar ratio was true for our 2nd dataset as well, with the dataset coming in at 98.162% accuracy, 100% precision. True positives for the second dataset came out to 921, true negatives came out to 286, which meant the model was performing decently well in both datasets. 

DTC (or the decision tree classifier machine learning algorithm) is good for data analytics and machine learning because they break down complex data into more manageable parts. While precision lingers around 81, the accuracy went up to 96. Still, based off of precision scores, the MNB did better. Both models classify the test message as a spam message accurately. You can see that when we employed the decision tree classifier instead of the multinomialNB, accuracy went to approximately 96%, and precision went down to about 81%. Overall, you can see that the multinomialNB worked out better than the DTC in terms of accuracy and precision. 

We experimented with the TFIDF algorithm. The TFIDF algorithm, or term frequency-inverse document frequency algorithm, is an algorithm that uses the frequency of words in order to find out how relevant the words are in a document or set of data. We experimented with using it as opposed to using CountVectorizer to see if our accuracy would be affected. Again, DTC is good for data analytics and machine learning because they break down complex data into more manageable parts. Accuracy went down to about 94 and precision went down to about 74. Changing from using the CountVectorizer to the TFIDF algorithm caused the accuracy to fall a bit down to 94.874%, and precision down to 74.194%. You may have also noticed that the number of false negatives and false positives went up. This was what we wanted to see. However, experimentation is necessary to understand, and so we also experimented with the decision tree classifier in our second dataset. 

Similar experimentation was performed on our second dataset. All of this experimentation had to lead to something. We wanted to find the best algorithms and models to answer the question of whether a message was a spam or ham message. We found the CountVectorizer aided us in higher accuracy and precision scores overall compared to the TFIDF. Furthermore, we found the MNB performed better for accuracy and precision than the DTC, which is what we expected based on what the MNB’s purpose is.


